{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP8_preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP8_preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import array\n",
    "import csv\n",
    "import gzip\n",
    "import logging\n",
    "import re\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "from collections import namedtuple\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import click\n",
    "import sentencepiece as spm\n",
    "import torch\n",
    "\n",
    "from datamaestro.data.csv import Generic as CSVData\n",
    "from datamaestro import prepare_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "#MAINDIR = Path(__file__).parent\n",
    "import os\n",
    "# Assuming this code is in the first cell of the notebook\n",
    "current_file = os.path.abspath(os.getcwd())\n",
    "MAINDIR = Path(current_file).parent\n",
    "\n",
    "\n",
    "RE_URL = re.compile(r\"(?:\\@|https?\\://)\\S+\")\n",
    "RE_MENTION = re.compile(r\"(?:@)\\S+\")\n",
    "RE_NOT  = re.compile('[^\\w\\s@:,;]+')\n",
    "\n",
    "def datareader(path: Path):\n",
    "    with open(path, \"rt\", encoding=\"utf-8\", errors='ignore') as fp:\n",
    "        for row in csv.reader(fp):\n",
    "            yield RE_NOT.sub(' ', RE_MENTION.sub('@', RE_URL.sub('',row[5]))), row[0]\n",
    "\n",
    "def cleanup(src, target):\n",
    "    \"\"\"Nettoyage du jeu de tweet\"\"\"\n",
    "    if not target.is_file():\n",
    "        logging.info(\"Creating the text data file from %s\", src)\n",
    "        target_tmp = target.with_suffix(\".tmp\")\n",
    "        with target_tmp.open(\"wt\", encoding=\"utf-8\") as out:\n",
    "            for tweet, klass in datareader(src):\n",
    "                out.write(tweet)\n",
    "                out.write(\"\\n\")\n",
    "\n",
    "        shutil.move(target_tmp, target)\n",
    "\n",
    "\n",
    "Batch = namedtuple(\"Batch\", [\"text\", \"labels\"])\n",
    "\n",
    "class TextDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, text: torch.LongTensor, sizes: torch.LongTensor, labels: torch.LongTensor):\n",
    "        self.text = text\n",
    "        self.sizes = sizes\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        return self.text[self.sizes[index]:self.sizes[index+1]], self.labels[index].item()\n",
    "\n",
    "    @staticmethod\n",
    "    def collate(batch):\n",
    "        data = [item[0] for item in batch]\n",
    "        labels = [item[1] for item in batch]\n",
    "        return Batch(torch.nn.utils.rnn.pad_sequence(data, batch_first=True), torch.LongTensor(labels))\n",
    "\n",
    "\n",
    "def process(mode: str, ds: CSVData, map: dict):\n",
    "    \"\"\"Process the dataset\n",
    "    \"\"\"\n",
    "    datapath = MAINDIR / f\"{mode}.pth\"\n",
    "    if datapath.is_file():\n",
    "        logging.info(\"Loading %s\", mode)\n",
    "        with gzip.open(datapath, \"rb\") as fp:\n",
    "            return torch.load(fp)\n",
    "\n",
    "    text = array.array('L')\n",
    "    sizes = array.array('L')\n",
    "    labels = array.array('B')\n",
    "    sizes.append(0)\n",
    "    for tweet, label in tqdm(datareader(ds.files[mode]), unit=\" sentences\"):\n",
    "        for tokenid in tokenizer.encode_as_ids(tweet):\n",
    "            text.append(tokenid)\n",
    "        sizes.append(len(text))\n",
    "        labels.append(int(label))\n",
    "\n",
    "    data = TextDataset(torch.LongTensor(text), torch.LongTensor(sizes), torch.LongTensor(labels))\n",
    "    with gzip.open(datapath, \"wb\") as fp:\n",
    "        torch.save(data, fp)\n",
    "    return data\n",
    "\n",
    "\n",
    "def generatedata(mode: str, tokenizer, vocab_size: int, data: CSVData, map):\n",
    "    datapath = MAINDIR / f\"{mode}-{vocab_size}.pth\"\n",
    "    if datapath.is_file():\n",
    "        return\n",
    "\n",
    "    text = array.array('L')\n",
    "    sizes = array.array('L')\n",
    "    labels = array.array('B')\n",
    "    sizes.append(0)\n",
    "    for tweet, label in tqdm(datareader(data.path), unit=\" sentences\"):\n",
    "        for tokenid in tokenizer.encode_as_ids(tweet):\n",
    "            text.append(tokenid)\n",
    "        label = int(label)\n",
    "        if label in map:\n",
    "            sizes.append(len(text))\n",
    "            labels.append(map[label])\n",
    "\n",
    "    data = TextDataset(torch.LongTensor(text), torch.LongTensor(sizes), torch.LongTensor(labels))\n",
    "    with gzip.open(datapath, \"wb\") as fp:\n",
    "        torch.save(data, fp)\n",
    "\n",
    "\n",
    "#@click.option(\"--vocab-size\", default=1000, type=int, help=\"Vocabulary size\")\n",
    "#@click.command()\n",
    "def cli(vocab_size):\n",
    "    # Création du jeu de données et du modèle\n",
    "    ds = prepare_dataset(\"com.sentiment140.english\")\n",
    "\n",
    "    # Création du vocabulaire\n",
    "    wpmodel = Path(\"wp{}.model\".format(vocab_size))\n",
    "    if not wpmodel.is_file():\n",
    "        logging.info(\"Did not find the wordpiece model %s\", wpmodel)\n",
    "        TRAINPATH = Path(\"sentiment140-train.txt\")\n",
    "        cleanup(ds.train.path, TRAINPATH)\n",
    "        logging.info(\"Création du vocabulaire avec sentencepiece\")\n",
    "        spm.SentencePieceTrainer.train(\n",
    "            input=str(TRAINPATH),\n",
    "            model_prefix=f\"wp{vocab_size}\",\n",
    "            vocab_size=vocab_size\n",
    "        )\n",
    "        TRAINPATH.unlink()\n",
    "\n",
    "\n",
    "    # Création des jeux de données\n",
    "    tokenizer = spm.SentencePieceProcessor()\n",
    "    tokenizer.Load(f\"wp{vocab_size}.model\")\n",
    "\n",
    "    CLASSMAP = { 0: 0, 4: 1 }\n",
    "    logging.info(\"Traitement du train/test (Sentiment 140)\")\n",
    "    generatedata(\"test\", tokenizer, vocab_size, ds.test, CLASSMAP)\n",
    "    generatedata(\"train\", tokenizer, vocab_size, ds.train, CLASSMAP)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if __name__ == \"__main__\":\n",
    "vocab_size=1000\n",
    "cli(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
