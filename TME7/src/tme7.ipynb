{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.functional import norm\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader,random_split,TensorDataset\n",
    "from pathlib import Path\n",
    "from datamaestro import prepare_dataset\n",
    "import time\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 311\n",
    "TRAIN_RATIO = 0.8\n",
    "LOG_PATH = \"/tmp/runs/lightning_logs\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Lit2Layer(pl.LightningModule):\n",
    "    def __init__(self,dim_in,l,dim_out,learning_rate=1e-3):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(nn.Linear(dim_in,l),nn.ReLU(),nn.Linear(l,dim_out))\n",
    "        self.learning_rate = learning_rate\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        self.name = \"exemple-lightning\"\n",
    "        self.valid_outputs = []\n",
    "        self.training_outputs = []\n",
    "\n",
    "    def forward(self,x):\n",
    "        \"\"\" Définit le comportement forward du module\"\"\"\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\" Définit l'optimiseur \"\"\"\n",
    "        optimizer = torch.optim.Adam(self.parameters(),lr=self.learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self,batch,batch_idx):\n",
    "        \"\"\" une étape d'apprentissage\n",
    "        doit retourner soit un scalaire (la loss),\n",
    "        soit un dictionnaire qui contient au moins la clé 'loss'\"\"\"\n",
    "        x, y = batch\n",
    "        yhat= self(x) ## equivalent à self.model(x)\n",
    "        loss = self.loss(yhat,y)\n",
    "        acc = (yhat.argmax(1)==y).sum()\n",
    "        logs = {\"loss\":loss,\"accuracy\":acc,\"nb\":len(x)}\n",
    "        self.log(\"accuracy\",acc/len(x),on_step=False,on_epoch=True)\n",
    "        self.valid_outputs.append({\"loss\":loss,\"accuracy\":acc,\"nb\":len(x)})\n",
    "        return logs\n",
    "\n",
    "    def validation_step(self,batch,batch_idx):\n",
    "        \"\"\" une étape de validation\n",
    "        doit retourner un dictionnaire\"\"\"\n",
    "        x, y = batch\n",
    "        yhat = self(x)\n",
    "        loss = self.loss(yhat,y)\n",
    "        acc = (yhat.argmax(1)==y).sum()\n",
    "        logs = {\"loss\":loss,\"accuracy\":acc,\"nb\":len(x)}\n",
    "        self.log(\"val_accuracy\", acc/len(x),on_step=False,on_epoch=True)\n",
    "        self.valid_outputs.append({\"loss\":loss,\"accuracy\":acc,\"nb\":len(x)})\n",
    "        return logs\n",
    "\n",
    "    def test_step(self,batch,batch_idx):\n",
    "        \"\"\" une étape de test \"\"\"\n",
    "        x, y = batch\n",
    "        yhat = self(x)\n",
    "        loss = self.loss(yhat,y)\n",
    "        acc = (yhat.argmax(1)==y).sum()\n",
    "        logs = {\"loss\":loss,\"accuracy\":acc,\"nb\":len(x)}\n",
    "        return logs\n",
    "\n",
    "    def log_x_end(self,outputs,phase):\n",
    "        total_acc = sum([o['accuracy'] for o in outputs])\n",
    "        total_nb = sum([o['nb'] for o in outputs])\n",
    "        total_loss = sum([o['loss'] for o in outputs])/len(outputs)\n",
    "        total_acc = total_acc/total_nb\n",
    "        self.log_dict({f\"loss/{phase}\":total_loss,f\"acc/{phase}\":total_acc})\n",
    "        #self.logger.experiment.add_scalar(f'loss/{phase}',total_loss,self.current_epoch)\n",
    "        #self.logger.experiment.add_scalar(f'acc/{phase}',total_acc,self.current_epoch)\n",
    "\n",
    "    def on_training_epoch_end(self):\n",
    "        \"\"\" hook optionel, si on a besoin de faire quelque chose apres une époque d'apprentissage.\n",
    "        Par exemple ici calculer des valeurs à logger\"\"\"\n",
    "        self.log_x_end(self.training_outputs,'train')\n",
    "        self.training_outputs.clear()\n",
    "        # Le logger de tensorboard est accessible directement avec self.logger.experiment.add_XXX\n",
    "    def on_validation_epoch_end(self):\n",
    "        \"\"\" hook optionel, si on a besoin de faire quelque chose apres une époque de validation.\"\"\"\n",
    "        self.log_x_end(self.valid_outputs,'valid')\n",
    "        self.valid_outputs.clear()\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitMnistData(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(self,batch_size=BATCH_SIZE,train_ratio=TRAIN_RATIO):\n",
    "        super().__init__()\n",
    "        self.dim_in = None\n",
    "        self.dim_out = None\n",
    "        self.batch_size = batch_size\n",
    "        self.train_ratio = train_ratio\n",
    "\n",
    "    def prepare_data(self):\n",
    "        ### Do not use \"self\" here.\n",
    "        prepare_dataset(\"com.lecun.mnist\")\n",
    "\n",
    "    def setup(self,stage=None):\n",
    "        ds = prepare_dataset(\"com.lecun.mnist\")\n",
    "        if stage ==\"fit\" or stage is None:\n",
    "            # Si on est en phase d'apprentissage\n",
    "            shape = ds.train.images.data().shape\n",
    "            self.dim_in = shape[1]*shape[2]\n",
    "            self.dim_out = len(set(ds.train.labels.data()))\n",
    "            ds_train = TensorDataset(torch.tensor(ds.train.images.data()).view(-1,self.dim_in).float()/255., torch.tensor(ds.train.labels.data()).long())\n",
    "            train_length = int(shape[0]*self.train_ratio)\n",
    "            self.mnist_train, self.mnist_val, = random_split(ds_train,[train_length,shape[0]-train_length])\n",
    "        if stage == \"test\" or stage is None:\n",
    "            # en phase de test\n",
    "            self.mnist_test= TensorDataset(torch.tensor(ds.test.images.data()).view(-1,self.dim_in).float()/255., torch.tensor(ds.test.labels.data()).long())\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.mnist_train,batch_size=self.batch_size)\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.mnist_val,batch_size=self.batch_size)\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.mnist_test,batch_size=self.batch_size)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = LitMnistData()\n",
    "data.prepare_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = data.train_dataloader()\n",
    "val_loader  = data.val_dataloader()\n",
    "test_loader = data.test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([311, 784]) torch.Size([311])\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_loader: \n",
    "    print(x.size(), y.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "model = Lit2Layer(data.dim_in,10,data.dim_out,learning_rate=1e-3)\n",
    "logger = TensorBoardLogger(save_dir=LOG_PATH,name=model.name,version=time.asctime(),default_hp_metric=False)\n",
    "trainer = pl.Trainer(\n",
    "    default_root_dir=LOG_PATH,\n",
    "    logger=logger,\n",
    "    max_epochs=100,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_15525/48696741.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "#trainer.fit(model,train_loader)\n",
    "#trainer.test(model,test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class Model2LP(nn.Module):\n",
    "    def __init__(self,dim_in,l,dim_out,learning_rate=1e-3):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(nn.Linear(dim_in,l),nn.ReLU(),nn.Linear(l,dim_out))\n",
    "        self.learning_rate = learning_rate\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        self.name = \"2 Layers Perceptron\"\n",
    "        self.valid_outputs = []\n",
    "        self.training_outputs = []\n",
    "\n",
    "    def forward(self,x):\n",
    "        \"\"\" Définit le comportement forward du module\"\"\"\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\" Définit l'optimiseur \"\"\"\n",
    "        optimizer = torch.optim.Adam(self.parameters(),lr=self.learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self,batch,batch_idx):\n",
    "        \"\"\" une étape d'apprentissage\n",
    "        doit retourner soit un scalaire (la loss),\n",
    "        soit un dictionnaire qui contient au moins la clé 'loss'\"\"\"\n",
    "        x, y = batch\n",
    "        yhat= self(x) ## equivalent à self.model(x)\n",
    "        loss = self.loss(yhat,y)\n",
    "        acc = (yhat.argmax(1)==y).sum()\n",
    "        logs = {\"loss\":loss,\"accuracy\":acc,\"nb\":len(x)}\n",
    "        # print(\"accuracy\",acc/len(x),on_step=False,on_epoch=True)\n",
    "        self.valid_outputs.append({\"loss\":loss,\"accuracy\":acc,\"nb\":len(x)})\n",
    "        return logs\n",
    "\n",
    "    def validation_step(self,batch,batch_idx):\n",
    "        \"\"\" une étape de validation\n",
    "        doit retourner un dictionnaire\"\"\"\n",
    "        x, y = batch\n",
    "        yhat = self(x)\n",
    "        loss = self.loss(yhat,y)\n",
    "        acc = (yhat.argmax(1)==y).sum()\n",
    "        logs = {\"loss\":loss,\"accuracy\":acc,\"nb\":len(x)}\n",
    "        print(\"val_accuracy\", acc/len(x),on_step=False,on_epoch=True)\n",
    "        self.valid_outputs.append({\"loss\":loss,\"accuracy\":acc,\"nb\":len(x)})\n",
    "        return logs\n",
    "\n",
    "    def test_step(self,batch,batch_idx):\n",
    "        \"\"\" une étape de test \"\"\"\n",
    "        x, y = batch\n",
    "        yhat = self(x)\n",
    "        loss = self.loss(yhat,y)\n",
    "        acc = (yhat.argmax(1)==y).sum()\n",
    "        logs = {\"loss\":loss,\"accuracy\":acc,\"nb\":len(x)}\n",
    "        return logs\n",
    "\n",
    "    def log_x_end(self,outputs,phase):\n",
    "        total_acc = sum([o['accuracy'] for o in outputs])\n",
    "        total_nb = sum([o['nb'] for o in outputs])\n",
    "        total_loss = sum([o['loss'] for o in outputs])/len(outputs)\n",
    "        total_acc = total_acc/total_nb\n",
    "        print({f\"loss/{phase}\":total_loss,f\"acc/{phase}\":total_acc})\n",
    "        #self.log_dict({f\"loss/{phase}\":total_loss,f\"acc/{phase}\":total_acc})\n",
    "        #self.logger.experiment.add_scalar(f'loss/{phase}',total_loss,self.current_epoch)\n",
    "        #self.logger.experiment.add_scalar(f'acc/{phase}',total_acc,self.current_epoch)\n",
    "\n",
    "    def on_training_epoch_end(self):\n",
    "        \"\"\" hook optionel, si on a besoin de faire quelque chose apres une époque d'apprentissage.\n",
    "        Par exemple ici calculer des valeurs à logger\"\"\"\n",
    "        self.log_x_end(self.training_outputs,'train')\n",
    "        self.training_outputs.clear()\n",
    "        # Le logger de tensorboard est accessible directement avec self.logger.experiment.add_XXX\n",
    "    def on_validation_epoch_end(self):\n",
    "        \"\"\" hook optionel, si on a besoin de faire quelque chose apres une époque de validation.\"\"\"\n",
    "        self.log_x_end(self.valid_outputs,'valid')\n",
    "        self.valid_outputs.clear()\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Phase apprentissage\n",
    "model = Model2LP(dim_in=784, l=200, dim_out=10)\n",
    "\n",
    "# Phase train\n",
    "v = True # verbose\n",
    "list_loss = []\n",
    "liste_acc = []\n",
    "for epoch in range(100):\n",
    "  loss_epoch = []\n",
    "  acc_epoch = []\n",
    "  for batch_idx, batch in enumerate(train_loader):\n",
    "    \n",
    "      logs = model.training_step(batch, batch_idx)\n",
    "\n",
    "      if v and batch_idx % 1 == 0:\n",
    "        courant = batch_idx * len(batch)\n",
    "        #print(f\"loss: {logs['loss']:>7f} acc: {logs['accuracy']:>7f} len: {logs['nb']:>7f}\")\n",
    "\n",
    "      loss_epoch.append(logs['loss'].detach())\n",
    "      acc_epoch.append(logs['accuracy'].detach())\n",
    "\n",
    "  list_loss.append(np.mean(loss_epoch))\n",
    "  liste_acc.append(np.mean(acc_epoch))\n",
    "\n",
    "    \n",
    "    \n",
    "plt.plot(list_loss,label=\"Loss_train\")\n",
    "plt.plot(liste_acc,label=\"Acc_train\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_in = 784\n",
    "dim_latent = 100\n",
    "dim_out = 10\n",
    "train_length =  50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "MLP3.__init__() takes 1 positional argument but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_16409/1758879225.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdim_latent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLP3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim_in\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim_latent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0msoft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_super_init\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m             raise TypeError(\"{}.__init__() takes 1 positional argument but {} were\"\n\u001b[0m\u001b[1;32m    450\u001b[0m                             \" given\".format(type(self).__name__, len(args) + 1))\n\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: MLP3.__init__() takes 1 positional argument but 4 were given"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "epoch = 10\n",
    "dim_latent = 100\n",
    "model = MLP3(dim_in,dim_latent,dim_out).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=1e-3)\n",
    "soft = nn.Softmax(dim=1)\n",
    "\n",
    "def train(train_loader, model, optimizer, epoch, device):\n",
    "    loss_train = []\n",
    "    loss_test = []\n",
    "    acc_train = []\n",
    "    acc_test = []\n",
    "    for e in range(epoch):\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in tqdm(enumerate(train_loader),total=len(train_loader)):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.cross_entropy(output,target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} '\n",
    "                    f'({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "                loss_train.append(loss.item())\n",
    "                acc_train.append((soft(output).argmax(1)==target).sum()/len(target))\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (data, target) in tqdm(enumerate(train_loader),total=len(test_loader)):\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                loss = F.cross_entropy(output,target)\n",
    "                if batch_idx % 10 == 0:\n",
    "                    print(f'Test Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} '\n",
    "                        f'({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "                    loss_test.append(loss.item())\n",
    "                    acc_test.append((soft(output).argmax(1)==target).sum()/len(target))\n",
    "    return loss_train, loss_test, acc_train, acc_test\n",
    "\n",
    "loss_train, loss_test, acc_train, acc_test = train(train_loader, model, optimizer, epoch, device)\n",
    "\n",
    "plt.plot(loss_train,label=\"Loss_train\")\n",
    "plt.plot(loss_test,label=\"Loss_test\")\n",
    "plt.plot(acc_train,label=\"Acc_train\")\n",
    "plt.plot(acc_test,label=\"Acc_test\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
